<?xml version="1.0" encoding="UTF-8"?>
<!--Autogenerated by Cloudera Manager-->
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://engine01:8020</value>
    </property>

    <!--  指定命名空间  -->
    <property>
        <name>dfs.nameservices</name>
        <value>engine01</value>
    </property>
    <!--  指定该命名空间下的两个机器作为我们的NameNode  -->
    <property>
        <name>dfs.ha.namenodes.engine01</name>
        <value>nn1,nn2</value>
    </property>

    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
    </property>

    <!-- 配置第一台服务器的namenode通信地址  -->
    <property>
        <name>dfs.namenode.rpc-address.engine01.nn1</name>
        <value>node1.hadoop02.aloudata.work:8020</value>
    </property>
    <!--  配置第二台服务器的namenode通信地址  -->
    <property>
        <name>dfs.namenode.rpc-address.engine01.nn2</name>
        <value>node2.hadoop02.aloudata.work:8020</value>
    </property>
    <!-- 所有从节点之间相互通信端口地址 -->
    <property>
        <name>dfs.namenode.servicerpc-address.engine01.nn1</name>
        <value>node1.hadoop02.aloudata.work:8022</value>
    </property>
    <!-- 所有从节点之间相互通信端口地址 -->
    <property>
        <name>dfs.namenode.servicerpc-address.engine01.nn2</name>
        <value>node2.hadoop02.aloudata.work:8022</value>
    </property>

    <!-- 第一台服务器namenode的web访问地址  -->
    <property>
        <name>dfs.namenode.http-address.engine01.nn1</name>
        <value>node1.hadoop02.aloudata.work:50070</value>
    </property>
    <!-- 第二台服务器namenode的web访问地址  -->
    <property>
        <name>dfs.namenode.http-address.engine01.nn2</name>
        <value>node2.hadoop02.aloudata.work:50070</value>
    </property>

    <!-- journalNode的访问地址，注意这个地址一定要配置 -->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>
            qjournal://node1.hadoop02.aloudata.work:8485;node2.hadoop02.aloudata.work:8485;node3.hadoop02.aloudata.work:8485/engine01
        </value>
    </property>
    <!--  指定故障自动恢复使用的哪个java类 -->
    <property>
        <name>dfs.client.failover.proxy.provider.engine01</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.automatic-failover.enabled.nameservice1</name>
        <value>true</value>
    </property>
    <!-- 故障转移使用的哪种通信机制 -->
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>

    <!-- 指定通信使用的公钥  -->
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/data/.id_rsa</value>
    </property>
    <!-- journalNode数据存放地址  -->
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/data/data/hadoop/dfs/jn</value>
    </property>
    <!-- 启用自动故障恢复功能 -->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <!-- namenode产生的文件存放路径 -->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///data/data/hadoop/dfs/nn/name</value>
    </property>
    <!-- edits产生的文件存放路径 -->
    <property>
        <name>dfs.namenode.edits.dir</name>
        <value>file:///data/data/hadoop/dfs/nn/edits</value>
    </property>
    <!-- dataNode文件存放路径 -->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///data/data/hadoop/dfs/dn</value>
    </property>
    <!-- 关闭hdfs的文件权限 -->
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <!-- 指定block文件块的大小 -->
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
    </property>
    <property>
        <name>dfs.namenode.acls.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.client.use.legacy.blockreader</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.domain.socket.path</name>
        <value>/var/run/hdfs-sockets/dn</value>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit.skip.checksum</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.client.domain.socket.data.traffic</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.block.access.token.enable</name>
        <value>true</value>
    </property>
    <property>
        <name>ipc.client.fallback-to-simple-auth-allowed</name>
        <value>true</value>
    </property>

    <!-- 添加配置 -->
    <property>
        <name>fs.hdfs.impl.disable.cache</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.image.transfer.timeout</name>
        <value>1200000</value>
    </property>

    <property>
        <name>parquet.metadata.read.parallelism</name>
        <value>4</value>
    </property>
    <property>
        <name>parquet.compression</name>
        <value>SNAPPY</value>
    </property>

    <property>
        <name>io.file.buffer.size</name>
        <value>65536</value>
    </property>

    <!-- hive config -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse/</value>
    </property>
</configuration>